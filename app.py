import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import os

# Autenticaci√≥n con Hugging Face
# Aseg√∫rate de reemplazar 'TU_TOKEN_AQUI' con tu token de acceso personal
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "TU_TOKEN_AQUI"

# Cargar el modelo y el tokenizador
model_name = "ITG/DialoGPT-medium-spanish-chitchat"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Configurar la p√°gina de Streamlit
st.set_page_config(page_title="Chatbot en Espa√±ol", page_icon="ü§ñ")
st.title("ü§ñ Chatbot en Espa√±ol")
st.write("Interact√∫a con un modelo de lenguaje en espa√±ol.")

# Inicializar el historial de conversaci√≥n
if "historial" not in st.session_state:
    st.session_state.historial = []

# Mostrar el historial de conversaci√≥n
for mensaje in st.session_state.historial:
    st.markdown(mensaje, unsafe_allow_html=True)

# Entrada del usuario
with st.form(key="formulario_chat"):
    entrada_usuario = st.text_input("Escribe tu mensaje:", "")
    enviar = st.form_submit_button("Enviar")

# Procesar la entrada del usuario
if enviar and entrada_usuario:
    # Mostrar el mensaje del usuario
    st.session_state.historial.append(f"<div style='color:blue'><b>T√∫:</b> {entrada_usuario}</div>")

    # Codificar la entrada del usuario
    entrada_ids = tokenizer.encode(entrada_usuario + tokenizer.eos_token, return_tensors="pt")

    # Generar la respuesta del modelo
    respuesta_ids = model.generate(
        entrada_ids,
        max_length=1000,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.7
    )

    # Decodificar y mostrar la respuesta del bot
    respuesta = tokenizer.decode(respuesta_ids[:, entrada_ids.shape[-1]:][0], skip_special_tokens=True)
    st.session_state.historial.append(f"<div style='color:green'><b>Bot:</b> {respuesta}</div>")
